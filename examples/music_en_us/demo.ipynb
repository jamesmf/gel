{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Union, Dict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example:\n",
    "    \"\"\"\n",
    "    class for holding the data for our example.\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str):\n",
    "        self.genres = pd.read_csv(os.path.join(path, \"band_genres.csv\"))\n",
    "        self.labels = pd.read_csv(os.path.join(path, \"band_labels.csv\"))\n",
    "        self.members = pd.read_csv(os.path.join(path, \"band_members.csv\"))\n",
    "\n",
    "    def graph_from_data(self) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Build a graph from our sparql queried wikidata music dataset\n",
    "        \"\"\"\n",
    "        g = nx.DiGraph()\n",
    "        for band, band_name, member, member_name in self.members.values:\n",
    "            g.add_node(band, name=band_name)\n",
    "            g.add_node(member, name=member_name)\n",
    "            g.add_edge(band, member, rel=\"has_member\")\n",
    "            g.add_edge(member, band, rel=\"member_of\")\n",
    "\n",
    "        for band, band_name, label in self.labels.values:\n",
    "            g.add_node(band, name=band_name)\n",
    "            g.add_node(label)\n",
    "            g.add_edge(label, band, rel=\"signed\")\n",
    "            g.add_edge(band, label, rel=\"signed_by\")\n",
    "\n",
    "        for band, band_name, genre, genre_name in self.genres.values:\n",
    "            g.add_node(band, name=band_name)\n",
    "            g.add_node(genre, name=genre_name)\n",
    "            g.add_edge(band, genre, rel=\"has_genre\")\n",
    "            g.add_edge(genre, band, rel=\"example_of\")\n",
    "\n",
    "        return g\n",
    "\n",
    "\n",
    "class Learner:\n",
    "    \"\"\"\n",
    "    Primary class for learning our graph embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, rel_name=\"rel\", min_relations=1, max_relations=6):\n",
    "        self.g: nx.Graph = graph\n",
    "        self.node_ids: Dict[str, int] = {\"\": 0}\n",
    "        self.node_id_inv: Dict[int, str] = {}\n",
    "        self.rel_ids: Dict[str, int] = {\"\": 0}\n",
    "        self.rel_id_inv: Dict[int, str] = {}\n",
    "        self.node_id_list = []\n",
    "        self.rel_name = rel_name\n",
    "        self.min_relations = min_relations\n",
    "        self.max_relations = max_relations\n",
    "\n",
    "    def fit_graph(self):\n",
    "        \"\"\"\n",
    "        Learn the mapping between nodes and their integer ids,\n",
    "        as well as relationship types and their ids\n",
    "        \"\"\"\n",
    "        for n1, n2, edge in self.g.edges(data=True):\n",
    "            rel = edge[self.rel_name]\n",
    "            if n1 not in self.node_ids:\n",
    "                self.node_ids[n1] = len(self.node_ids)\n",
    "            if n2 not in self.node_ids:\n",
    "                self.node_ids[n2] = len(self.node_ids)\n",
    "            if rel not in self.rel_ids:\n",
    "                self.rel_ids[rel] = len(self.rel_ids)\n",
    "        self.node_id_list = [i for i in self.node_ids.keys() if i != \"\"]\n",
    "        self.node_id_inv = {v: k for k, v in self.node_ids.items()}\n",
    "        self.rel_id_inv = {v: k for k, v in self.rel_ids.items()}\n",
    "\n",
    "    def node_to_features(self, node_id):\n",
    "        \"\"\"\n",
    "        Given a node on the graph, featurize its relations and\n",
    "        return its identity\n",
    "        \"\"\"\n",
    "        node = self.g[node_id]\n",
    "        node_ind = self.node_ids[node_id]\n",
    "        n_relations = np.random.randint(self.min_relations, self.max_relations + 1)\n",
    "        keys = list(node.keys())\n",
    "        random.shuffle(keys)\n",
    "        keys = keys[:n_relations]\n",
    "        key_ids = np.array([self.node_ids[n] for n in keys])\n",
    "        rel_ids = np.array([self.rel_ids[node[n2][self.rel_name]] for n2 in keys])\n",
    "        return (key_ids, rel_ids), node_ind\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        x_zero = []\n",
    "        x_one = []\n",
    "        y = []\n",
    "        for n in range(0, batch_size):\n",
    "            node_id = random.choice(self.node_id_list)\n",
    "            xsub, ysub = self.node_to_features(node_id)\n",
    "            x_zero.append(xsub[0])\n",
    "            x_one.append(xsub[1])\n",
    "            y.append(ysub)\n",
    "        mx = np.max([len(i) for i in x_zero])\n",
    "        return (\n",
    "            (\n",
    "                krs.preprocessing.sequence.pad_sequences(x_zero, mx),\n",
    "                krs.preprocessing.sequence.pad_sequences(x_one, mx),\n",
    "            ),\n",
    "            np.array(y),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our example data\n",
    "ex = Example('data/')\n",
    "\n",
    "# turn it into a networkx graph\n",
    "music = ex.graph_from_data()\n",
    "\n",
    "# fit the Learner on our dataset\n",
    "l = Learner(music)\n",
    "l.fit_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we define our model. All of this will move to its own class with wrappers\n",
    "for training and running queries.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as krs\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    # embedding layers\n",
    "    node_embedding_in = krs.layers.Embedding(len(l.node_ids) + 1, EMBEDDING_DIM)\n",
    "    rel_embedding = krs.layers.Embedding(len(l.rel_ids) + 1, EMBEDDING_DIM)\n",
    "    # vars for loss\n",
    "    output_weights = tf.Variable(tf.random.normal([len(l.node_ids) + 1, EMBEDDING_DIM]))\n",
    "    output_biases = tf.Variable(tf.zeros([len(l.node_ids) + 1]))\n",
    "\n",
    "\n",
    "def get_embedding(x):\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        # Lookup the corresponding embedding vectors for each sample in X.\n",
    "        drop1 = krs.layers.Dropout(0.25)\n",
    "        drop2 = krs.layers.Dropout(0.25)\n",
    "        x_node_embed = drop1(node_embedding_in(x[0]))\n",
    "        x_rel_embed = drop2(rel_embedding(x[1]))\n",
    "        # multiply the relationship embeddings by the node embeddings\n",
    "        mul = tf.math.multiply(x_node_embed,x_rel_embed)\n",
    "        # max pool\n",
    "        out = tf.math.reduce_max(mul, axis=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_loss(x_inp, y):\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        y_true = tf.cast(tf.reshape(y, [-1, 1]), tf.int64)\n",
    "        y = tf.cast(y, tf.int64)\n",
    "        samp = tf.random.uniform_candidate_sampler(y_true, 1, 128, True, len(l.node_ids) + 1)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sampled_softmax_loss(\n",
    "                weights=output_weights,\n",
    "                biases=output_biases,\n",
    "                labels=y_true,\n",
    "                inputs=x_inp,\n",
    "                num_sampled=128,\n",
    "                num_classes=len(l.node_ids) + 1,\n",
    "                num_true=1,\n",
    "                sampled_values = samp,\n",
    "            )\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "\n",
    "def run_optimization(x, y, optimizer):\n",
    "    y = tf.convert_to_tensor(y)\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "        with tf.GradientTape() as g:\n",
    "            emb = get_embedding(x)\n",
    "            loss = get_loss(emb, y)\n",
    "\n",
    "        # Compute gradients.\n",
    "        to_diff = node_embedding_in.weights + rel_embedding.weights + [output_weights, output_biases]\n",
    "        gradients = g.gradient(loss, to_diff)\n",
    "\n",
    "        # Update W and b following gradients.\n",
    "        optimizer.apply_gradients(zip(gradients, to_diff))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def evaluate(x_embed):\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        # Compute the cosine similarity between input data embedding and every embedding vectors\n",
    "        x_embed = tf.cast(x_embed, tf.float32)\n",
    "        x_embed_norm = x_embed / tf.sqrt(tf.reduce_sum(tf.square(x_embed)))\n",
    "        embedding_norm = output_weights / tf.sqrt(\n",
    "            tf.reduce_sum(tf.square(output_weights), 1, keepdims=True), tf.float32\n",
    "        )\n",
    "        cosine_sim_op = tf.matmul(x_embed_norm, embedding_norm, transpose_b=True)\n",
    "        return cosine_sim_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training loop.\n",
    "\n",
    "Sample sets of relationships from sampled nodes and minimize their sampled_softmax_loss on\n",
    "all the nodes in the graph.\n",
    "\n",
    "Occasionally evaluate the most-similar output node vectors to our predictions\n",
    "\"\"\"\n",
    "PRINT_EVERY = 1000\n",
    "EVAL_EVERY = 10000\n",
    "INCREASE_BS_EVERY = 2000\n",
    "LEARNING_RATE = 0.5\n",
    "BATCH_SIZE = 4\n",
    "MAX_BATCH_SIZE = 256\n",
    "\n",
    "# increase batch size linearly\n",
    "def increase_bs(bs, mx):\n",
    "    return np.min([bs + 4, mx])\n",
    "\n",
    "optimizer = tf.optimizers.SGD(LEARNING_RATE)\n",
    "\n",
    "# monitor loss\n",
    "train_losses = []\n",
    "bs = BATCH_SIZE\n",
    "for ep in tqdm(range(0, 1000000)):\n",
    "    train_losses.append(run_optimization(*(l.get_batch(bs)), optimizer))\n",
    "    \n",
    "    if ep % PRINT_EVERY == 0:\n",
    "        print(f\"training loss at epoch {ep}: {np.mean(train_losses)}\")\n",
    "        train_losses = train_losses[-20:]\n",
    "        \n",
    "    if (ep % INCREASE_BS_EVERY == 0) and ep != 0:\n",
    "        increase_bs(bs, MAX_BATCH_SIZE)\n",
    "\n",
    "    # run an evaluation loop; pick random nodes+relations and find the most similar output nodes\n",
    "    if (ep % EVAL_EVERY == 0) and ep != 0:\n",
    "        batch_x, batch_y = l.get_batch(bs)\n",
    "        sim = evaluate(get_embedding(batch_x)).numpy()\n",
    "        for i in range(3):\n",
    "            top_k = 3  # number of nearest neighbors.\n",
    "            nearest = (-sim[i, :]).argsort()[:top_k]\n",
    "            print(f\"for node {l.node_id_inv[int(batch_y[i])]}- {music.nodes[l.node_id_inv[int(batch_y[i])]]} closest are nodes:\")\n",
    "\n",
    "            for k in range(top_k):\n",
    "                print(music.nodes[l.node_id_inv[nearest[k]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Traveling Wilburys'} 0.6650992\n"
     ]
    }
   ],
   "source": [
    "# ask the model a question\n",
    "def get_query(node_names, query_nodes):\n",
    "    query_nodes = [\n",
    "        l.node_ids[\n",
    "            [\n",
    "                i\n",
    "                for i in music.nodes\n",
    "                if \"name\" in music.nodes[i] and music.nodes[i][\"name\"] == z\n",
    "            ][0]\n",
    "        ]\n",
    "        for z in node_names\n",
    "    ]\n",
    "\n",
    "    query_rels = [l.rel_ids[z] for z in relationships]\n",
    "    return query_nodes, query_rels\n",
    "\n",
    "# \"what band has the relationship 'has_member' with both Tom Petty and Roy Orbison\"\n",
    "node_names = [\"Tom Petty\", \"Roy Orbison\"]\n",
    "relationships = [\"has_member\", \"has_member\"]\n",
    "\n",
    "query_nodes, query_rels = get_query(node_names, relationships)\n",
    "batch_x = [np.array(query_nodes).reshape(-1, 1),\n",
    "            np.array(query_rels).reshape(-1, 1)]\n",
    "\n",
    "# get most similar output vectors\n",
    "sim = evaluate(get_embedding(batch_x)).numpy()\n",
    "top_k = 1\n",
    "nearest = (-sim[0, :]).argsort()[:top_k]\n",
    "nearest_scores = [sim[0, j] for j in nearest]\n",
    "\n",
    "for k in range(top_k):\n",
    "    print(music.nodes[l.node_id_inv[nearest[k]]], nearest_scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Allman Brothers Band'} 0.6447925\n"
     ]
    }
   ],
   "source": [
    "# what about genres?\n",
    "node_names = [\"classic rock\"]\n",
    "relationships = [\"has_genre\"]\n",
    "\n",
    "query_nodes, query_rels = get_query(node_names, relationships)\n",
    "\n",
    "batch_x = [np.array(query_nodes).reshape(-1, 1),\n",
    "            np.array(query_rels).reshape(-1, 1)]\n",
    "\n",
    "sim = evaluate(get_embedding(batch_x)).numpy()\n",
    "top_k = 1  # number of nearest neighbors.\n",
    "nearest = (-sim[0, :]).argsort()[:top_k]\n",
    "nearest_scores = [sim[0, j] for j in nearest]\n",
    "\n",
    "for k in range(top_k):\n",
    "    print(music.nodes[l.node_id_inv[nearest[k]]], nearest_scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
